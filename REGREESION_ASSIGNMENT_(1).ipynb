{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#REGRESSION ASSIGNMENT"
      ],
      "metadata": {
        "id": "acv4N0KxKEq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SOLUTION"
      ],
      "metadata": {
        "id": "d1rpqCHWKLWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1- What is Simple Linear Regression?"
      ],
      "metadata": {
        "id": "B5snJFHaKRvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A1- Simple Linear Regression is a statistical method used to model the relationship between two continuous variables:\n",
        "\n",
        "One independent variable (X) – also called the predictor or input.\n",
        "\n",
        "One dependent variable (Y) – also called the response or output."
      ],
      "metadata": {
        "id": "u_o7cuRvKYzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2- What are the key assumptions of Simple Linear Regression?"
      ],
      "metadata": {
        "id": "3IfQyBh3KmFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A2- ChatGPT said:\n",
        "Simple Linear Regression relies on several key assumptions to ensure that the results (coefficients, predictions, etc.) are valid and reliable"
      ],
      "metadata": {
        "id": "SHRVcj_KKqje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3- What does the coefficient m represent in the equation Y=mX+c?"
      ],
      "metadata": {
        "id": "A7VJywnFKx4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A3- In the linear equation:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑚\n",
        "𝑋\n",
        "+\n",
        "𝑐\n",
        "Y=mX+c\n",
        "the coefficient\n",
        "𝑚\n",
        "m represents the slope of the line.\n",
        "\n"
      ],
      "metadata": {
        "id": "xejPfJrbK3Wi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4- What does the intercept c represent in the equation Y=mX+c."
      ],
      "metadata": {
        "id": "5_4y7p0SLJNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A4- In the equation:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑚\n",
        "𝑋\n",
        "+\n",
        "𝑐\n",
        "Y=mX+c\n",
        "the coefficient\n",
        "𝑐\n",
        "c represents the intercept (also called the Y-intercept).\n",
        "\n"
      ],
      "metadata": {
        "id": "GnALUVoNLOy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5- How do we calculate the slope m in Simple Linear Regression?"
      ],
      "metadata": {
        "id": "V-sHkZHaLX7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A6- To calculate the slope\n",
        "𝑚\n",
        "m in Simple Linear Regression, we use a statistical formula that determines the best-fitting line through the data points.\n",
        "\n",
        "📌 Formula for the Slope\n",
        "𝑚\n",
        "m:\n",
        "𝑚\n",
        "=\n",
        "∑\n",
        "(\n",
        "𝑋\n",
        "𝑖\n",
        "−\n",
        "𝑋\n",
        "ˉ\n",
        ")\n",
        "(\n",
        "𝑌\n",
        "𝑖\n",
        "−\n",
        "𝑌\n",
        "ˉ\n",
        ")\n",
        "∑\n",
        "(\n",
        "𝑋\n",
        "𝑖\n",
        "−\n",
        "𝑋\n",
        "ˉ\n",
        ")\n",
        "2\n",
        "m=\n",
        "∑(X\n",
        "i\n",
        "​\n",
        " −\n",
        "X\n",
        "ˉ\n",
        " )\n",
        "2\n",
        "\n",
        "∑(X\n",
        "i\n",
        "​\n",
        " −\n",
        "X\n",
        "ˉ\n",
        " )(Y\n",
        "i\n",
        "​\n",
        " −\n",
        "Y\n",
        "ˉ\n",
        " )\n",
        "​\n",
        "\n"
      ],
      "metadata": {
        "id": "jc6rYggBLdi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6-What is the purpose of the least squares method in Simple Linear Regression?"
      ],
      "metadata": {
        "id": "A1sAX_CQLtBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A6-The purpose of the Least Squares Method in Simple Linear Regression is to find the best-fitting line through a set of data points by minimizing the total error."
      ],
      "metadata": {
        "id": "bwDi6j0LL02u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7- How is the coefficient of determination (R²) interpreted in Simple Linear Regression"
      ],
      "metadata": {
        "id": "7A1V2pSAL-C2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A7- The coefficient of determination, denoted as\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        " , is a key metric in Simple Linear Regression that tells you how well the regression line fits the data.\n",
        "\n",
        "📘 Definition:\n",
        "𝑅\n",
        "2\n",
        "=\n",
        "Explained Variation\n",
        "Total Variation\n",
        "R\n",
        "2\n",
        " =\n",
        "Total Variation\n",
        "Explained Variation\n",
        "​\n"
      ],
      "metadata": {
        "id": "SdWY3Ik0MFJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8- What is Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "Z8gVTCPuMTg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A8- Multiple Linear Regression (MLR) is an extension of Simple Linear Regression that models the relationship between one dependent variable (Y) and two or more independent variables (X₁, X₂, ..., Xₙ)."
      ],
      "metadata": {
        "id": "P8NXJKrGMZOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9- What is the main difference between Simple and Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "4FE1MeffMgxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A9- Simple Linear Regression uses one independent variable to predict the dependent variable, while Multiple Linear Regression uses two or more independent variables.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M9w2PzDTMmoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10- What are the key assumptions of Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "UdWW56zFMzEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A10- The key assumptions of multiple linear regression include linearity, independence, homoscedasticity, and normality. These assumptions ensure reliable results in regression analysis."
      ],
      "metadata": {
        "id": "3zrEqItIM4oN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11- What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression mode?"
      ],
      "metadata": {
        "id": "C7JpNkSAM_tz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A11- Heteroscedasticity in a multiple linear regression model refers to the non-constant variance of the error terms (residuals) across different values of the independent variables. This violates one of the core assumptions of ordinary least squares (OLS) regression, which assumes that the error terms have a constant variance (homoscedasticity). When heteroscedasticity is present, the OLS regression model can lead to biased and unreliable results."
      ],
      "metadata": {
        "id": "Y2Ta3tqCNGp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12- How can you improve a Multiple Linear Regression model with high multicollinearity?"
      ],
      "metadata": {
        "id": "2aqkjYI5NPqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A12- To address multicollinearity in a multiple linear regression model, you can consider several strategies, including removing highly correlated variables, combining them, or using techniques like Principal Component Analysis (PCA) to reduce dimensionality and mitigate the issue according to Analytics Vidhya."
      ],
      "metadata": {
        "id": "B5ymkBb7NVPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13- What are some common techniques for transforming categorical variables for use in regression model?"
      ],
      "metadata": {
        "id": "FiLgGUIHNc5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A13- Several techniques can transform categorical variables for use in regression models. One-hot encoding (also called dummy coding) is a common method, creating binary variables for each category. Label encoding assigns a unique integer to each category, but it can introduce an arbitrary order that may not be meaningful. Ordinal encoding is suitable for categorical variables with a natural order, while binary encoding uses a unique binary representation for each category"
      ],
      "metadata": {
        "id": "ijhlncQnNjtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14- What is the role of interaction terms in Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "_hNKZOJsNxL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A14- In multiple linear regression, interaction terms represent a non-additive relationship between two or more independent variables and the dependent variable. They allow the model to capture situations where the effect of one predictor on the outcome changes depending on the level of another predictor"
      ],
      "metadata": {
        "id": "ymAvJITxN3Jz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15- How can the interpretation of intercept differ between Simple and Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "Y23Q2XW7N9EU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A15- In both simple and multiple linear regression, the intercept represents the predicted value of the dependent variable when all independent variables are zero. However, the context and meaning of the intercept can differ depending on whether the model includes multiple predictors. In multiple linear regression, the intercept is the expected value of the response variable when all predictors are zero, while in simple linear regression, it's the predicted value when the single predictor is zero"
      ],
      "metadata": {
        "id": "FSPOG9UeOEIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16- What is the significance of the slope in regression analysis, and how does it affect predictions"
      ],
      "metadata": {
        "id": "0vQPe_tEOQr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A16- Chapter 8. Regression Basics – Introductory Business ...In regression analysis, the slope represents the rate of change in the dependent variable (Y) for every one-unit change in the independent variable (X), holding all other factors constant. A positive slope indicates a positive relationship (as X increases, Y tends to increase), while a negative slope indicates a negative relationship (as X increases, Y tends to decrease). The steeper the slope, the stronger the linear relationship"
      ],
      "metadata": {
        "id": "nbX1tqUgOXlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17-  How does the intercept in a regression model provide context for the relationship between variables?"
      ],
      "metadata": {
        "id": "SvHqm1VDOibh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A17- The intercept in a regression model represents the expected value of the dependent variable when all independent variables are zero. It provides a baseline or starting point for the relationship, helping to anchor the model and giving context to how the dependent variable behaves without the influence of predictors."
      ],
      "metadata": {
        "id": "JJQPSdozOotl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18- What are the limitations of using R² as a sole measure of model performance?"
      ],
      "metadata": {
        "id": "oBtsfq0FO2sB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A18- R² can be a useful metric for assessing model performance, but it has limitations when used alone. It primarily measures the proportion of variance in the dependent variable explained by the independent variables, but it doesn't indicate how well the model fits the data's structure or capture non-linear relationships. Additionally, R² can be sensitive to the inclusion of irrelevant variables, potentially leading to overfitting."
      ],
      "metadata": {
        "id": "GEByb7_8O7-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19- - How would you interpret a large standard error for a regression coefficient?"
      ],
      "metadata": {
        "id": "i6h5v4PxPDI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A19- A large standard error for a regression coefficient means there is a lot of uncertainty about the true value of that coefficient. It suggests that the estimate is less precise, and the variable may not be a reliable predictor of the dependent variable.\n",
        "\n",
        "In practical terms:\n",
        "\n",
        "The confidence interval for the coefficient will be wide.\n",
        "\n",
        "It may be harder to conclude that the coefficient is significantly different from zero.\n",
        "\n",
        "The variable might not have a strong or consistent effect on the outcome.\n",
        "\n"
      ],
      "metadata": {
        "id": "6Z6Gr52pPVpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20-  How can heteroscedasticity be identified in residual plots, and why is it important to address it?"
      ],
      "metadata": {
        "id": "PZOP13DGPd6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A20- Heteroscedasticity in residual plots can be identified by observing a fan-shaped pattern where the spread of residuals (points on the plot) increases or decreases systematically as the fitted values (predicted values) change"
      ],
      "metadata": {
        "id": "RXh-aPQAPmQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21- What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²"
      ],
      "metadata": {
        "id": "nht7uhqcPvkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A21- A high R-squared and low adjusted R-squared in a multiple linear regression model suggest that the model might be overfitted, meaning it's capturing noise in the data rather than true relationships. While R-squared indicates a good fit with the added variables, the adjusted R-squared, which penalizes for including unnecessary predictors, shows that these added variables might not be providing significant improvements in the model's predictive power."
      ],
      "metadata": {
        "id": "vLEYGaG5P0kj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22- Why is it important to scale variables in Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "yuTKyVUkP7BI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A22- Scaling variables in multiple linear regression is beneficial for several reasons, including faster convergence of optimization algorithms like gradient descent, improved interpretability of coefficients, and preventing dominant features from overshadowing others, according to GeeksforGeeks and Medium."
      ],
      "metadata": {
        "id": "IHEA9wR8QAQU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23- What is polynomial regression?"
      ],
      "metadata": {
        "id": "7NleCS_wQFaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A23- Polynomial regression is a type of regression analysis where the relationship between a dependent variable and one or more independent variables is modeled as an nth-degree polynomial"
      ],
      "metadata": {
        "id": "ZU_TtGoFQKpY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24- How does polynomial regression differ from linear regression?"
      ],
      "metadata": {
        "id": "opTRLCZVQRfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A24- Polynomial regression differs from linear regression in the type of relationship it models between variables. Linear regression fits a straight line to the data, while polynomial regression fits a curve, allowing for more complex, non-linear relationships. This difference arises from how the independent variables are used in the model; polynomial regression uses powers and products of the independent variables to create a polynomial equation"
      ],
      "metadata": {
        "id": "Q3V7ZArfQW2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25- When is polynomial regression used?"
      ],
      "metadata": {
        "id": "VCu8vPH0QgfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A25- Polynomial regression is used when the relationship between variables is non-linear and can be modeled by a polynomial function, such as a quadratic, cubic, or higher-degree curve"
      ],
      "metadata": {
        "id": "RfukWZgKQmHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q26- What is the general equation for polynomial regression?"
      ],
      "metadata": {
        "id": "yy-bSV8gQtRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A26- Understanding Polynomial Regression!!! | by Abhigyan ...The general equation for polynomial regression, where y is the dependent variable and x is the independent variable, is y = β₀ + β₁x + β₂x² + ... + βₙxⁿ + ϵ"
      ],
      "metadata": {
        "id": "OPzcT7I9Q9XV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q27- Can polynomial regression be applied to multiple variables?"
      ],
      "metadata": {
        "id": "WKyFCvraRJj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A27- Yes, polynomial regression can be applied to multiple variables. In this case, it's referred to as \"multivariate polynomial regression\". This allows the model to capture not just the relationships between individual variables and the outcome, but also potential interactions between them"
      ],
      "metadata": {
        "id": "0R7HaAykRZ7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q28- What are the limitations of polynomial regression?"
      ],
      "metadata": {
        "id": "50RVT-BFRcgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A28- Polynomial regression, while powerful for modeling complex relationships, has limitations. One main issue is the risk of overfitting, especially with high-degree polynomials, where the model learns the training data too well and performs poorly on unseen data"
      ],
      "metadata": {
        "id": "-ADAfXX1RhpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q29- What methods can be used to evaluate model fit when selecting the degree of a polynomial?"
      ],
      "metadata": {
        "id": "LncWZATsRspj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A29-Polynomial regression, while powerful for modeling complex relationships, has limitations. One main issue is the risk of overfitting, especially with high-degree polynomials, where the model learns the training data too well and performs poorly on unseen data. Another limitation is the computational complexity and the challenge of selecting the optimal polynomial degree, which can impact model performance. Furthermore, polynomial regression can be more sensitive to outliers than linear regression"
      ],
      "metadata": {
        "id": "0MUbNfzHRz5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q30- Why is visualization important in polynomial regression?"
      ],
      "metadata": {
        "id": "o4RrlKQ0R9Vv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A30- Visualization is crucial in polynomial regression because it helps understand the model's fit to the data and identify potential issues like overfitting or underfitting. By visualizing the predicted values against the actual values, you can see how well the model captures the relationship between the variables"
      ],
      "metadata": {
        "id": "3ch2QF7ZSDwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q31- - How is polynomial regression implemented in Python?"
      ],
      "metadata": {
        "id": "QhDwlWz5SNTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A31- Polynomial regression is implemented in Python using libraries such as NumPy, scikit-learn, and matplotlib. The process involves transforming the input features into polynomial features and then fitting a linear regression model to these transformed features."
      ],
      "metadata": {
        "id": "SMsHDKbzSPi8"
      }
    }
  ]
}